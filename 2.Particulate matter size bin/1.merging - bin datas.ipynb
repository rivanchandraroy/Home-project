{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import pytz\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(source_dir, location, type, target_dir):\n",
    "    print(f'Merging for {location} : {type}')\n",
    "    print('='*50)\n",
    "\n",
    "    path = f\"{source_dir}/{location}/{type}\"\n",
    "\n",
    "    # Print the path for debugging\n",
    "    print(f\"Looking for CSV files in: {path}\")\n",
    "\n",
    "    # Use glob to get all the CSV files in the directory\n",
    "    all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "\n",
    "    # Initialize an empty list to hold the DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    # Loop through the list of files and read each file into a DataFrame\n",
    "    print(\"Reading file...\")\n",
    "    for file in tqdm(all_files):\n",
    "        try:\n",
    "            df = pd.read_csv(file, encoding='ascii')\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    # Check if df_list is still empty after the loop\n",
    "    if len(df_list) == 0:\n",
    "        raise ValueError(\"No DataFrames were created. Check if the CSV files are valid and readable.\")\n",
    "\n",
    "    # Concatenate all the DataFrames in the list into a single DataFrame\n",
    "    try:\n",
    "        merged_df = pd.concat(df_list, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error concatenating DataFrames: {e}\")\n",
    "\n",
    "    ## Taking cols related to time, meteorological parameters and pm2.5\n",
    "    # necessary_cols = ['UTCDateTime', 'current_humidity', 'current_temp_f', 'current_dewpoint_f', 'pressure', 'pm2_5_cf_1', 'pm2_5_atm', 'pm2_5_cf_1_b', 'pm2_5_atm_b']\n",
    "    # merged_df = merged_df[necessary_cols].copy()\n",
    "\n",
    "    # # New algorithm\n",
    "    # def calculate_average_with_error_check(value1, value2, threshold=0.10): ## Updated logic\n",
    "    #     avg = (value1 + value2)/2\n",
    "    #     if avg < 5: avg = np.nan\n",
    "    #     if avg > 1000 : avg = np.nan   ## initially taking upper limit of 1000. It was farther readjusted to 500 afterwards\n",
    "    #     if value1 > 100:\n",
    "    #         if value1 == 0 or value2 == 0:\n",
    "    #             error = float('inf')\n",
    "    #         else:\n",
    "    #             error = abs(value1 - value2) / value1\n",
    "    #         if error <= threshold:\n",
    "    #             return avg\n",
    "    #         else:\n",
    "    #             return np.nan\n",
    "    #     else:\n",
    "    #         if np.absolute(value1-value2) <= 10:\n",
    "    #             return avg\n",
    "    #         else:\n",
    "    #             return np.nan\n",
    "        \n",
    "\n",
    "    # merged_df['pm2_5_atm_avg'] = merged_df.apply(\n",
    "    #         lambda row: calculate_average_with_error_check(row['pm2_5_atm'], row['pm2_5_atm_b']), axis=1)\n",
    "\n",
    "\n",
    "    # merged_df['pm2_5_cf_1_avg'] = merged_df.apply(\n",
    "    #         lambda row: calculate_average_with_error_check(row['pm2_5_cf_1'], row['pm2_5_cf_1_b']), axis=1)\n",
    "\n",
    "    os.makedirs(f'{target_dir}/{location}', exist_ok=True)\n",
    "    try:\n",
    "        merged_df.to_csv(f'{target_dir}/{location}/{type}.csv', index = False)\n",
    "    except Exception as e:\n",
    "        print(\"Error in saving file...\")\n",
    "        print(e)\n",
    "        while True:\n",
    "            print(\"1. Retry\")\n",
    "            print(\"2. Skip\")\n",
    "            option = int(input())\n",
    "            if option == 1:\n",
    "                try: \n",
    "                    merged_df.to_csv(f'Data/Raw files/{location}/{type}_merged.csv', index = False)\n",
    "                except Exception as e:\n",
    "                    print(\"Error again\", e)\n",
    "                    continue\n",
    "            elif option == 2:\n",
    "                break\n",
    "            else:\n",
    "                print(\"invalid input\")\n",
    "                continue\n",
    "\n",
    "\n",
    "    print(f'file saved : Data/Raw files/{location}/{type}_merged.csv')\n",
    "\n",
    "    # length = merged_df.shape[0]\n",
    "    # atmdata = length - merged_df['pm2_5_atm_avg'].isna().sum()\n",
    "    # cfdata = length - merged_df['pm2_5_cf_1_avg'].isna().sum()\n",
    "\n",
    "    # return atmdata/length*100, cfdata/length*100, length\n",
    "\n",
    "\n",
    "# merge(\"Ajimpur home data\", \"indoor type No 12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging for H1 : Indoor\n",
      "==================================================\n",
      "Looking for CSV files in: data/H1/Indoor\n",
      "Reading file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b03064e32448d18d3a2737b2cf64fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved : Data/Raw files/H1/Indoor_merged.csv\n",
      "Merging for H1 : Outdoor\n",
      "==================================================\n",
      "Looking for CSV files in: data/H1/Outdoor\n",
      "Reading file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aeeb4fafbff4b768821481c74029757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved : Data/Raw files/H1/Outdoor_merged.csv\n",
      "Merging for H2 : Indoor\n",
      "==================================================\n",
      "Looking for CSV files in: data/H2/Indoor\n",
      "Reading file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62380be867834f9da570e44ec96a79ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved : Data/Raw files/H2/Indoor_merged.csv\n",
      "Merging for H2 : Outdoor\n",
      "==================================================\n",
      "Looking for CSV files in: data/H2/Outdoor\n",
      "Reading file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cd0f0ee6234f349c8f309356d2b78c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved : Data/Raw files/H2/Outdoor_merged.csv\n",
      "Merging for H3 : Indoor\n",
      "==================================================\n",
      "Looking for CSV files in: data/H3/Indoor\n",
      "Reading file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564cddb34cba45b1bbc448c1dac7dd6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/167 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved : Data/Raw files/H3/Indoor_merged.csv\n",
      "Merging for H3 : Outdoor\n",
      "==================================================\n",
      "Looking for CSV files in: data/H3/Outdoor\n",
      "Reading file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0223595f794333b65f75b2f23d8ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/163 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved : Data/Raw files/H3/Outdoor_merged.csv\n",
      "Merging for H4 : Indoor\n",
      "==================================================\n",
      "Looking for CSV files in: data/H4/Indoor\n",
      "Reading file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67d17af2dd54d029b3cb34891eecd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved : Data/Raw files/H4/Indoor_merged.csv\n",
      "Merging for H4 : Outdoor\n",
      "==================================================\n",
      "Looking for CSV files in: data/H4/Outdoor\n",
      "Reading file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f974ca19c79945c1b5329a75add97bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved : Data/Raw files/H4/Outdoor_merged.csv\n",
      "Merging for H5 : Indoor\n",
      "==================================================\n",
      "Looking for CSV files in: data/H5/Indoor\n",
      "Reading file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4dd0bad44f4a349ac59c030c82f99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved : Data/Raw files/H5/Indoor_merged.csv\n",
      "Merging for H5 : Outdoor\n",
      "==================================================\n",
      "Looking for CSV files in: data/H5/Outdoor\n",
      "Reading file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901820642e5c458cbb3dd190126220a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved : Data/Raw files/H5/Outdoor_merged.csv\n",
      "Merging for H6 : Indoor\n",
      "==================================================\n",
      "Looking for CSV files in: data/H6/Indoor\n",
      "Reading file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df7960192644aa884b3118c2ef625a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved : Data/Raw files/H6/Indoor_merged.csv\n",
      "Merging for H6 : Outdoor\n",
      "==================================================\n",
      "Looking for CSV files in: data/H6/Outdoor\n",
      "Reading file...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8514e9fdc8c4117a9b5ae6b1ad1f0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved : Data/Raw files/H6/Outdoor_merged.csv\n"
     ]
    }
   ],
   "source": [
    "source_dir = 'data'\n",
    "target_dir = 'merged'\n",
    "locations = [f for f in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, f))]\n",
    "\n",
    "\n",
    "for location in locations:\n",
    "    types = [\"Indoor\", \"Outdoor\"]\n",
    "    for type in types:\n",
    "        merge(source_dir, location, type, target_dir)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(all_in_one)",
   "language": "python",
   "name": "all_in_one"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
